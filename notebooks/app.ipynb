{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4fa47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# JEWELLERY MULTIMODAL SEARCH BACKEND (FASTAPI)\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca19ed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTS\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import FileResponse\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bdbdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "\n",
    "BASE_DIR = \"/home/akash/Jewellary_RAG/backend\"\n",
    "\n",
    "CHROMA_PATH = os.path.join(BASE_DIR, \"chroma\")   # <- chroma_primary\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\", \"tanishq\")\n",
    "IMAGE_DIR = os.path.join(DATA_DIR, \"images\")\n",
    "BLIP_CAPTIONS_PATH = os.path.join(DATA_DIR, \"blip_captions.json\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab05d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD MODELS (ONCE)\n",
    "# ============================================================\n",
    "\n",
    "print(\"ðŸ”¹ Loading CLIP model...\")\n",
    "clip_model, _ = clip.load(\"ViT-B/16\", device=DEVICE)\n",
    "clip_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49736c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”¹ Loading BLIP captions...\")\n",
    "with open(BLIP_CAPTIONS_PATH, \"r\") as f:\n",
    "    BLIP_CAPTIONS = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d011d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD CHROMA (PERSISTED DB)\n",
    "# ============================================================\n",
    "\n",
    "print(\"ðŸ”¹ Connecting to Chroma DB...\")\n",
    "chroma_client = chromadb.Client(\n",
    "    Settings(\n",
    "        persist_directory=CHROMA_PATH,\n",
    "        anonymized_telemetry=False\n",
    "    )\n",
    ")\n",
    "\n",
    "image_collection = chroma_client.get_collection(\"jewelry_images\")\n",
    "metadata_collection = chroma_client.get_collection(\"jewelry_metadata\")\n",
    "\n",
    "print(\n",
    "    \"âœ… Chroma loaded | Images:\",\n",
    "    image_collection.count(),\n",
    "    \"| Metadata:\",\n",
    "    metadata_collection.count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea5075f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FASTAPI APP\n",
    "# ============================================================\n",
    "\n",
    "app = FastAPI(title=\"Jewellery Multimodal Search\")\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a747d9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# REQUEST / RESPONSE SCHEMAS\n",
    "# ============================================================\n",
    "\n",
    "class TextSearchRequest(BaseModel):\n",
    "    query: str\n",
    "    top_k: int = 5\n",
    "\n",
    "\n",
    "class SimilarSearchRequest(BaseModel):\n",
    "    image_id: str\n",
    "    top_k: int = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a345d49",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CLIP QUERY ENCODING (TEXT ONLY)\n",
    "# ============================================================\n",
    "\n",
    "def encode_text_clip(text: str) -> np.ndarray:\n",
    "    tokens = clip.tokenize([text]).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        emb = clip_model.encode_text(tokens)\n",
    "        emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "    return emb.cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8685de3a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INTENT & ATTRIBUTE DETECTION (LIGHT, SAFE)\n",
    "# ============================================================\n",
    "\n",
    "def detect_intent_and_attributes(query: str) -> Dict:\n",
    "    q = query.lower()\n",
    "    attrs = {}\n",
    "\n",
    "    if \"necklace\" in q:\n",
    "        attrs[\"category\"] = \"necklace\"\n",
    "    elif \"ring\" in q:\n",
    "        attrs[\"category\"] = \"ring\"\n",
    "\n",
    "    if \"gold\" in q:\n",
    "        attrs[\"metal\"] = \"gold\"\n",
    "    elif \"silver\" in q:\n",
    "        attrs[\"metal\"] = \"silver\"\n",
    "\n",
    "    if \"pearl\" in q:\n",
    "        attrs[\"primary_stone\"] = \"pearl\"\n",
    "    elif \"diamond\" in q:\n",
    "        attrs[\"primary_stone\"] = \"diamond\"\n",
    "\n",
    "    return {\n",
    "        \"intent\": \"search\",\n",
    "        \"attributes\": attrs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9276a6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUAL RETRIEVAL (NO LANGCHAIN)\n",
    "# ============================================================\n",
    "\n",
    "def retrieve_visual_candidates(query_text: str, k: int = 100):\n",
    "    q_emb = encode_text_clip(query_text)\n",
    "\n",
    "    res = image_collection.query(\n",
    "        query_embeddings=[q_emb],\n",
    "        n_results=k\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"image_id\": img_id,\n",
    "            \"visual_score\": dist\n",
    "        }\n",
    "        for img_id, dist in zip(res[\"ids\"][0], res[\"distances\"][0])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ee3845",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# METADATA SCORING REFINEMENTS\n",
    "# ============================================================\n",
    "\n",
    "def adaptive_alpha(query_attrs: Dict) -> float:\n",
    "    return 0.1 + 0.1 * len(query_attrs)\n",
    "\n",
    "\n",
    "def refined_metadata_adjustment(meta: Dict, query_attrs: Dict) -> float:\n",
    "    score = 0.0\n",
    "\n",
    "    for attr, q_val in query_attrs.items():\n",
    "        m_val = meta.get(attr)\n",
    "        conf = meta.get(f\"confidence_{attr}\", 0.0)\n",
    "\n",
    "        if m_val == q_val:\n",
    "            score += conf\n",
    "        elif conf > 0.6:\n",
    "            score -= 0.3 * conf\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def apply_metadata_boost(candidates: List[Dict], query_attrs: Dict):\n",
    "    alpha = adaptive_alpha(query_attrs)\n",
    "    ranked = []\n",
    "\n",
    "    for c in candidates:\n",
    "        meta = metadata_collection.get(\n",
    "            ids=[c[\"image_id\"]],\n",
    "            include=[\"metadatas\"]\n",
    "        )[\"metadatas\"][0]\n",
    "\n",
    "        adjust = refined_metadata_adjustment(meta, query_attrs)\n",
    "        final_score = c[\"visual_score\"] - alpha * adjust\n",
    "\n",
    "        ranked.append({\n",
    "            \"image_id\": c[\"image_id\"],\n",
    "            \"visual_score\": c[\"visual_score\"],\n",
    "            \"metadata_boost\": adjust,\n",
    "            \"final_score\": final_score\n",
    "        })\n",
    "\n",
    "    return sorted(ranked, key=lambda x: x[\"final_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba15f0c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LLM-FREE EXPLANATION (SAFE, GROUNDED)\n",
    "# ============================================================\n",
    "\n",
    "def explain_match(image_id: str, query_attrs: Dict) -> str:\n",
    "    caption = BLIP_CAPTIONS.get(image_id, \"\")\n",
    "    meta = metadata_collection.get(\n",
    "        ids=[image_id],\n",
    "        include=[\"metadatas\"]\n",
    "    )[\"metadatas\"][0]\n",
    "\n",
    "    reasons = []\n",
    "\n",
    "    for k, v in query_attrs.items():\n",
    "        if meta.get(k) == v:\n",
    "            reasons.append(v)\n",
    "\n",
    "    if reasons:\n",
    "        return (\n",
    "            f\"Recommended because it visually resembles a {meta['category']} \"\n",
    "            f\"and matches attributes such as {', '.join(reasons)}.\"\n",
    "        )\n",
    "\n",
    "    return \"Recommended due to strong visual similarity.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be180f93",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# API ENDPOINTS\n",
    "# ============================================================\n",
    "\n",
    "@app.post(\"/search/text\")\n",
    "def search_text(req: TextSearchRequest):\n",
    "    intent = detect_intent_and_attributes(req.query)\n",
    "    attrs = intent[\"attributes\"]\n",
    "\n",
    "    candidates = retrieve_visual_candidates(req.query, k=100)\n",
    "    ranked = apply_metadata_boost(candidates, attrs)[:req.top_k]\n",
    "\n",
    "    results = []\n",
    "    for r in ranked:\n",
    "        results.append({\n",
    "            \"image_id\": r[\"image_id\"],\n",
    "            \"explanation\": explain_match(r[\"image_id\"], attrs),\n",
    "            \"scores\": {\n",
    "                \"visual\": r[\"visual_score\"],\n",
    "                \"metadata\": r[\"metadata_boost\"],\n",
    "                \"final\": r[\"final_score\"]\n",
    "            }\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"query\": req.query,\n",
    "        \"intent\": attrs,\n",
    "        \"results\": results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2395975",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@app.post(\"/search/similar\")\n",
    "def search_similar(req: SimilarSearchRequest):\n",
    "    base = image_collection.get(\n",
    "        ids=[req.image_id],\n",
    "        include=[\"embeddings\"]\n",
    "    )[\"embeddings\"][0]\n",
    "\n",
    "    res = image_collection.query(\n",
    "        query_embeddings=[base],\n",
    "        n_results=req.top_k + 1\n",
    "    )\n",
    "\n",
    "    base_meta = metadata_collection.get(\n",
    "        ids=[req.image_id],\n",
    "        include=[\"metadatas\"]\n",
    "    )[\"metadatas\"][0]\n",
    "\n",
    "    attrs = {\n",
    "        k: base_meta[k]\n",
    "        for k in [\"category\", \"metal\", \"primary_stone\"]\n",
    "        if base_meta.get(k) != \"unknown\"\n",
    "    }\n",
    "\n",
    "    candidates = [\n",
    "        {\n",
    "            \"image_id\": img_id,\n",
    "            \"visual_score\": dist\n",
    "        }\n",
    "        for img_id, dist in zip(res[\"ids\"][0], res[\"distances\"][0])\n",
    "        if img_id != req.image_id\n",
    "    ]\n",
    "\n",
    "    ranked = apply_metadata_boost(candidates, attrs)[:req.top_k]\n",
    "\n",
    "    results = []\n",
    "    for r in ranked:\n",
    "        results.append({\n",
    "            \"image_id\": r[\"image_id\"],\n",
    "            \"explanation\": explain_match(r[\"image_id\"], attrs),\n",
    "            \"scores\": {\n",
    "                \"visual\": r[\"visual_score\"],\n",
    "                \"metadata\": r[\"metadata_boost\"],\n",
    "                \"final\": r[\"final_score\"]\n",
    "            }\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"base_image\": req.image_id,\n",
    "        \"results\": results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce2e661",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/image/{image_id}\")\n",
    "def get_image(image_id: str):\n",
    "    path = os.path.join(IMAGE_DIR, image_id)\n",
    "    if not os.path.exists(path):\n",
    "        raise HTTPException(status_code=404, detail=\"Image not found\")\n",
    "    return FileResponse(path)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
