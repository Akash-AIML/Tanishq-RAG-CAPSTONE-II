{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89a82240",
   "metadata": {},
   "source": [
    "# üíé Jewellery Multimodal Search Backend (FastAPI)\n",
    "\n",
    "**Purpose**: Production-ready FastAPI backend for multimodal jewellery search\n",
    "\n",
    "**Features**:\n",
    "- CLIP-based visual similarity search\n",
    "- LLM-powered intent detection (Groq Llama)\n",
    "- Cross-encoder re-ranking for semantic accuracy\n",
    "- OCR-based query extraction (NVIDIA NeMo + GPT-4.1-Nano fallback)\n",
    "- Metadata filtering and boosting\n",
    "- ChromaDB vector storage\n",
    "\n",
    "**Architecture**:\n",
    "1. Text/Image ‚Üí CLIP Encoding\n",
    "2. Vector Search (ChromaDB)\n",
    "3. Metadata Filtering\n",
    "4. Cross-Encoder Re-ranking\n",
    "5. LLM Explanation Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac972d",
   "metadata": {},
   "source": [
    "## üì¶ Imports and Dependencies\n",
    "\n",
    "Core libraries for:\n",
    "- **FastAPI**: Web framework\n",
    "- **CLIP**: Multimodal embeddings\n",
    "- **ChromaDB**: Vector database\n",
    "- **Groq/OpenAI**: LLM services\n",
    "- **Sentence Transformers**: Cross-encoder re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a57513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "import chromadb\n",
    "\n",
    "from fastapi import FastAPI, HTTPException, File, UploadFile\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import FileResponse\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "import base64\n",
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdb785b",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration and Path Setup\n",
    "\n",
    "**Auto-detection logic**:\n",
    "- If `/app/data` exists ‚Üí Running in Docker (Hugging Face Spaces)\n",
    "- Otherwise ‚Üí Running locally (use script directory)\n",
    "\n",
    "**Key paths**:\n",
    "- `CHROMA_PATH`: Persisted vector database\n",
    "- `DATA_DIR`: Tanishq dataset directory\n",
    "- `IMAGE_DIR`: Product images\n",
    "- `BLIP_CAPTIONS_PATH`: Pre-generated image captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858bafd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect if running in Docker (HF Spaces) or locally\n",
    "if os.path.exists(\"/app/data\"):\n",
    "    # Running in Docker (Hugging Face Spaces)\n",
    "    BASE_DIR = \"/app\"\n",
    "else:\n",
    "    # Running locally - use script directory as base\n",
    "    BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "CHROMA_PATH = os.path.join(BASE_DIR, \"chroma_primary\")\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\", \"tanishq\")\n",
    "IMAGE_DIR = os.path.join(DATA_DIR, \"images\")\n",
    "BLIP_CAPTIONS_PATH = os.path.join(DATA_DIR, \"blip_captions.json\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341e9976",
   "metadata": {},
   "source": [
    "## üöÄ Lazy Model Loading (Cold Start Optimization)\n",
    "\n",
    "**Strategy**: Load models only on first use to reduce startup time\n",
    "\n",
    "**Models**:\n",
    "- **CLIP ViT-B/16**: Visual-text embeddings (~350MB)\n",
    "- **Cross-Encoder**: Semantic re-ranking (~90MB)\n",
    "\n",
    "**Benefits**:\n",
    "- Faster API startup\n",
    "- Lower memory footprint for unused features\n",
    "- Better for serverless deployments (HF Spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927f6bdb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Global model references (loaded on first use)\n",
    "clip_model = None\n",
    "cross_encoder = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12db31b2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_clip_model():\n",
    "    \"\"\"Lazy load CLIP model on first use\"\"\"\n",
    "    global clip_model\n",
    "    if clip_model is None:\n",
    "        print(\"üîπ Loading CLIP model...\")\n",
    "        model, _ = clip.load(\"ViT-B/16\", device=DEVICE)\n",
    "        model.eval()\n",
    "        clip_model = model\n",
    "        print(\"‚úÖ CLIP model loaded\")\n",
    "    return clip_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6c691e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_cross_encoder():\n",
    "    \"\"\"Lazy load Cross-Encoder on first use\"\"\"\n",
    "    global cross_encoder\n",
    "    if cross_encoder is None:\n",
    "        print(\"üîπ Loading Cross-Encoder for re-ranking...\")\n",
    "        cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "        print(\"‚úÖ Cross-Encoder loaded\")\n",
    "    return cross_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d18059",
   "metadata": {},
   "source": [
    "## üìù Load BLIP Captions\n",
    "\n",
    "Pre-generated image captions using BLIP model.\n",
    "Used for cross-encoder re-ranking to provide textual context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4885b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîπ Loading BLIP captions...\")\n",
    "with open(BLIP_CAPTIONS_PATH, \"r\") as f:\n",
    "    BLIP_CAPTIONS = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af16ca3e",
   "metadata": {},
   "source": [
    "## ü§ñ Initialize LLM Clients\n",
    "\n",
    "**Primary LLM**: Groq (Llama 3.3 70B)\n",
    "- Intent detection\n",
    "- Attribute extraction\n",
    "- Result explanations\n",
    "\n",
    "**OCR Services**:\n",
    "- **Primary**: NVIDIA NeMo Retriever OCR\n",
    "- **Fallback**: GPT-4.1-Nano (Navigate Labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f23c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\n",
    "if GROQ_API_KEY:\n",
    "    print(\"üîπ Initializing Groq LLM client...\")\n",
    "    groq_client = OpenAI(\n",
    "        base_url=\"https://api.groq.com/openai/v1\",\n",
    "        api_key=GROQ_API_KEY\n",
    "    )\n",
    "else:\n",
    "    groq_client = None\n",
    "    print(\"‚ö†Ô∏è GROQ_API_KEY not set; LLM features disabled (fallbacks enabled)\")\n",
    "\n",
    "# NVIDIA OCR API configuration\n",
    "NVIDIA_API_KEY = os.environ.get(\"NVIDIA_API_KEY\")\n",
    "NVIDIA_OCR_URL = \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-ocr-v1\"\n",
    "\n",
    "# Fallback OCR configuration\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2b847e",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è Load ChromaDB (Persisted Vector Database)\n",
    "\n",
    "**Collections**:\n",
    "- `jewelry_images`: CLIP image embeddings (512-dim vectors)\n",
    "- `jewelry_metadata`: Product metadata (category, metal, stone, etc.)\n",
    "\n",
    "**Why separate collections?**\n",
    "- Image embeddings optimized for vector search\n",
    "- Metadata optimized for filtering and boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297186b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîπ Connecting to Chroma DB...\")\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "\n",
    "image_collection = chroma_client.get_collection(\"jewelry_images\")\n",
    "metadata_collection = chroma_client.get_collection(\"jewelry_metadata\")\n",
    "\n",
    "print(\n",
    "    \"‚úÖ Chroma loaded | Images:\",\n",
    "    image_collection.count(),\n",
    "    \"| Metadata:\",\n",
    "    metadata_collection.count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49961339",
   "metadata": {},
   "source": [
    "## üåê FastAPI Application Setup\n",
    "\n",
    "**CORS Configuration**:\n",
    "- Localhost development (port 5173)\n",
    "- Vercel deployments (*.vercel.app)\n",
    "- Ngrok tunnels (for testing)\n",
    "\n",
    "**Middleware**:\n",
    "- Request timeout (60s)\n",
    "- Upload size limit (5MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb142db",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI(title=\"Jewellery Multimodal Search\")\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\n",
    "        \"http://localhost:5173\",\n",
    "        \"https://tanishq-rag-capstone-1lj2x4y1v-akash-aimls-projects.vercel.app\",  # Vercel preview\n",
    "        \"https://*.vercel.app\",  # All Vercel deployments\n",
    "        \"https://*.ngrok-free.app\",  # Allow ngrok tunnels\n",
    "        \"https://*.ngrok-free.dev\",  # Allow ngrok tunnels (new domain)\n",
    "        \"https://*.ngrok.io\",         # Allow ngrok tunnels (legacy)\n",
    "    ],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2aad3c",
   "metadata": {},
   "source": [
    "## ‚è±Ô∏è Request Optimization Middleware\n",
    "\n",
    "**Purpose**: Prevent resource exhaustion on Hugging Face Spaces\n",
    "\n",
    "**Protections**:\n",
    "- Upload size limit: 5MB (prevents memory issues)\n",
    "- Request timeout: 60s (prevents hanging requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254ac387",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from starlette.requests import Request\n",
    "\n",
    "@app.middleware(\"http\")\n",
    "async def add_optimizations(request: Request, call_next):\n",
    "    \"\"\"Add upload size limits and request timeouts\"\"\"\n",
    "    \n",
    "    # Limit upload size to 5MB\n",
    "    if request.method == \"POST\":\n",
    "        content_length = request.headers.get(\"content-length\")\n",
    "        if content_length and int(content_length) > 5 * 1024 * 1024:  # 5MB\n",
    "            raise HTTPException(status_code=413, detail=\"File too large (max 5MB)\")\n",
    "    \n",
    "    # Add request timeout (60s for local dev/slower machines)\n",
    "    try:\n",
    "        response = await asyncio.wait_for(call_next(request), timeout=60.0)\n",
    "        return response\n",
    "    except asyncio.TimeoutError:\n",
    "        raise HTTPException(status_code=504, detail=\"Request timeout (max 60s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e7b63e",
   "metadata": {},
   "source": [
    "## üìã Request/Response Schemas (Pydantic Models)\n",
    "\n",
    "**TextSearchRequest**:\n",
    "- `query`: User search text\n",
    "- `filters`: Explicit UI filters (e.g., {\"metal\": \"gold\"})\n",
    "- `top_k`: Number of results\n",
    "- `use_reranking`: Enable cross-encoder (slower but more accurate)\n",
    "- `use_explanations`: Enable LLM explanations (adds ~500ms)\n",
    "\n",
    "**SimilarSearchRequest**:\n",
    "- `image_id`: Base image for similarity search\n",
    "- `top_k`: Number of similar items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c216c5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TextSearchRequest(BaseModel):\n",
    "    query: str\n",
    "    filters: Dict[str, str] = None  # Explicit UI filters (e.g. {\"metal\": \"gold\"})\n",
    "    top_k: int = 5\n",
    "    use_reranking: bool = True  # Toggle cross-encoder (3x faster when False)\n",
    "    use_explanations: bool = True  # Toggle LLM explanations (500ms+ faster when False)\n",
    "\n",
    "\n",
    "class SimilarSearchRequest(BaseModel):\n",
    "    image_id: str\n",
    "    top_k: int = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35068ac5",
   "metadata": {},
   "source": [
    "## üî§ CLIP Text Encoding\n",
    "\n",
    "**Function**: `encode_text_clip(text: str) -> np.ndarray`\n",
    "\n",
    "**Process**:\n",
    "1. Tokenize text with CLIP tokenizer\n",
    "2. Encode to 512-dim embedding\n",
    "3. L2 normalize (cosine similarity)\n",
    "4. Clean up GPU memory\n",
    "\n",
    "**Memory optimization**:\n",
    "- Explicit tensor deletion\n",
    "- CUDA cache clearing (if GPU available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f62f5a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def encode_text_clip(text: str) -> np.ndarray:\n",
    "    \"\"\"Encode text using CLIP with memory cleanup\"\"\"\n",
    "    model = get_clip_model()\n",
    "    tokens = clip.tokenize([text]).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        emb = model.encode_text(tokens)\n",
    "        emb = emb / emb.norm(dim=-1, keepdim=True)\n",
    "        result = emb.cpu().numpy()[0]\n",
    "    \n",
    "    # Memory cleanup\n",
    "    del tokens, emb\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c1b5a2",
   "metadata": {},
   "source": [
    "## üß† LLM-Powered Intent & Attribute Detection\n",
    "\n",
    "**Function**: `detect_intent_and_attributes(query: str) -> Dict`\n",
    "\n",
    "**Purpose**: Extract structured search attributes from natural language\n",
    "\n",
    "**Schema**:\n",
    "```json\n",
    "{\n",
    "  \"intent\": \"search\",\n",
    "  \"attributes\": {\n",
    "    \"category\": \"ring|necklace|earring|bracelet|null\",\n",
    "    \"metal\": \"gold|silver|platinum|null\",\n",
    "    \"primary_stone\": \"diamond|pearl|ruby|emerald|sapphire|null\"\n",
    "  },\n",
    "  \"exclusions\": {\n",
    "    \"category\": \"...\",\n",
    "    \"metal\": \"...\",\n",
    "    \"primary_stone\": \"...\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Examples**:\n",
    "- \"gold ring with diamonds\" ‚Üí attributes: {category: ring, metal: gold, stone: diamond}\n",
    "- \"ring with no diamonds\" ‚Üí attributes: {category: ring}, exclusions: {stone: diamond}\n",
    "- \"plain silver necklace\" ‚Üí attributes: {category: necklace, metal: silver}, exclusions: {stone: any}\n",
    "\n",
    "**Fallback**: Simple keyword matching if LLM unavailable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7ae5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_intent_and_attributes(query: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract search attributes and exclusions from query using LLM with fixed schema.\n",
    "    \n",
    "    Returns:\n",
    "        {\n",
    "            \"intent\": \"search\",\n",
    "            \"attributes\": {category, metal, primary_stone},  # Items to INCLUDE\n",
    "            \"exclusions\": {category, metal, primary_stone}   # Items to EXCLUDE\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Extract jewellery search attributes from this query.\n",
    "\n",
    "Query: \"{query}\"\n",
    "\n",
    "Return ONLY valid JSON with this exact schema:\n",
    "{{\n",
    "  \"intent\": \"search\",\n",
    "  \"attributes\": {{\n",
    "    \"category\": \"ring|necklace|earring|bracelet|null\",\n",
    "    \"metal\": \"gold|silver|platinum|null\",\n",
    "    \"primary_stone\": \"diamond|pearl|ruby|emerald|sapphire|null\"\n",
    "  }},\n",
    "  \"exclusions\": {{\n",
    "    \"category\": \"ring|necklace|earring|bracelet|null\",\n",
    "    \"metal\": \"gold|silver|platinum|null\",\n",
    "    \"primary_stone\": \"diamond|pearl|ruby|emerald|sapphire|null\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Rules:\n",
    "- \"attributes\" = what to INCLUDE (positive filters)\n",
    "- \"exclusions\" = what to EXCLUDE (negative filters)\n",
    "- Use null for unspecified fields\n",
    "- Detect negations: \"no\", \"without\", \"not\", \"plain\", \"-free\"\n",
    "\n",
    "Examples:\n",
    "\n",
    "Query: \"gold ring with diamonds\"\n",
    "{{\"intent\": \"search\", \"attributes\": {{\"category\": \"ring\", \"metal\": \"gold\", \"primary_stone\": \"diamond\"}}, \"exclusions\": {{\"category\": null, \"metal\": null, \"primary_stone\": null}}}}\n",
    "\n",
    "Query: \"ring with no diamonds\"\n",
    "{{\"intent\": \"search\", \"attributes\": {{\"category\": \"ring\", \"metal\": null, \"primary_stone\": null}}, \"exclusions\": {{\"category\": null, \"metal\": null, \"primary_stone\": \"diamond\"}}}}\n",
    "\n",
    "Query: \"plain silver necklace\"\n",
    "{{\"intent\": \"search\", \"attributes\": {{\"category\": \"necklace\", \"metal\": \"silver\", \"primary_stone\": null}}, \"exclusions\": {{\"category\": null, \"metal\": null, \"primary_stone\": \"any\"}}}}\n",
    "\n",
    "Query: \"gold necklace without pearls\"\n",
    "{{\"intent\": \"search\", \"attributes\": {{\"category\": \"necklace\", \"metal\": \"gold\", \"primary_stone\": null}}, \"exclusions\": {{\"category\": null, \"metal\": null, \"primary_stone\": \"pearl\"}}}}\n",
    "\n",
    "Return ONLY the JSON, no explanation.\"\"\"\n",
    "\n",
    "    def simple_fallback() -> Dict:\n",
    "        q = query.lower()\n",
    "        attrs = {}\n",
    "\n",
    "        if \"necklace\" in q:\n",
    "            attrs[\"category\"] = \"necklace\"\n",
    "        elif \"ring\" in q:\n",
    "            attrs[\"category\"] = \"ring\"\n",
    "\n",
    "        if \"gold\" in q:\n",
    "            attrs[\"metal\"] = \"gold\"\n",
    "        elif \"silver\" in q:\n",
    "            attrs[\"metal\"] = \"silver\"\n",
    "\n",
    "        if \"pearl\" in q:\n",
    "            attrs[\"primary_stone\"] = \"pearl\"\n",
    "        elif \"diamond\" in q:\n",
    "            attrs[\"primary_stone\"] = \"diamond\"\n",
    "\n",
    "        return {\n",
    "            \"intent\": \"search\",\n",
    "            \"attributes\": attrs,\n",
    "            \"exclusions\": {}\n",
    "        }\n",
    "\n",
    "    if groq_client is None:\n",
    "        return simple_fallback()\n",
    "\n",
    "    try:\n",
    "        response = groq_client.chat.completions.create(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        result_text = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Extract JSON from response (handle markdown code blocks)\n",
    "        if \"```json\" in result_text:\n",
    "            result_text = result_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in result_text:\n",
    "            result_text = result_text.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "        \n",
    "        result = json.loads(result_text)\n",
    "        \n",
    "        # Clean null values\n",
    "        result[\"attributes\"] = {k: v for k, v in result.get(\"attributes\", {}).items() if v and v != \"null\"}\n",
    "        result[\"exclusions\"] = {k: v for k, v in result.get(\"exclusions\", {}).items() if v and v != \"null\"}\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è LLM extraction failed: {e}, falling back to simple extraction\")\n",
    "        return simple_fallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481eece7",
   "metadata": {},
   "source": [
    "## üîç Visual Retrieval Pipeline\n",
    "\n",
    "**Function**: `retrieve_visual_candidates(query_text, k, where_filter)`\n",
    "\n",
    "**Process**:\n",
    "1. Encode query text with CLIP\n",
    "2. Query ChromaDB image collection\n",
    "3. Return top-K candidates with similarity scores\n",
    "\n",
    "**Note**: No metadata filtering at this stage (done separately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0844cc9c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def retrieve_visual_candidates(query_text: str, k: int = 100, where_filter: Dict = None):\n",
    "    q_emb = encode_text_clip(query_text)\n",
    "\n",
    "    # Use Chroma's built-in filtering if provided\n",
    "    res = image_collection.query(\n",
    "        query_embeddings=[q_emb],\n",
    "        n_results=k,\n",
    "        where=where_filter\n",
    "    )\n",
    "\n",
    "    if not res[\"ids\"] or not res[\"ids\"][0]:\n",
    "        return []\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"image_id\": img_id,\n",
    "            \"visual_score\": dist\n",
    "        }\n",
    "        for img_id, dist in zip(res[\"ids\"][0], res[\"distances\"][0])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2e87d2",
   "metadata": {},
   "source": [
    "## üìä Metadata Scoring and Boosting\n",
    "\n",
    "**Strategy**: Combine visual similarity with metadata matching\n",
    "\n",
    "**Functions**:\n",
    "- `adaptive_alpha()`: Dynamic weight for metadata boost\n",
    "- `refined_metadata_adjustment()`: Score based on attribute matches\n",
    "- `apply_metadata_boost()`: Final ranking with exclusion filtering\n",
    "\n",
    "**Exclusion Logic**:\n",
    "- Hard filter: Completely remove excluded items\n",
    "- \"any\" exclusion: Remove items with ANY value for that attribute\n",
    "- Specific exclusion: Remove items matching exact value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71e0aeb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def adaptive_alpha(query_attrs: Dict) -> float:\n",
    "    return 0.1 + 0.1 * len(query_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90006cd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def refined_metadata_adjustment(meta: Dict, query_attrs: Dict) -> float:\n",
    "    score = 0.0\n",
    "\n",
    "    for attr, q_val in query_attrs.items():\n",
    "        m_val = meta.get(attr)\n",
    "        conf = meta.get(f\"confidence_{attr}\", 0.0)\n",
    "\n",
    "        if m_val == q_val:\n",
    "            score += conf\n",
    "        elif conf > 0.6:\n",
    "            score -= 0.3 * conf\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710ae98b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def apply_metadata_boost(candidates: List[Dict], query_attrs: Dict, exclusions: Dict = None):\n",
    "    \"\"\"\n",
    "    Rank candidates by combining visual similarity with metadata matching.\n",
    "    HARD FILTER out excluded items completely.\n",
    "    \n",
    "    Args:\n",
    "        candidates: List of {image_id, visual_score}\n",
    "        query_attrs: Attributes to INCLUDE (boost matching items)\n",
    "        exclusions: Attributes to EXCLUDE (FILTER OUT completely)\n",
    "    \"\"\"\n",
    "    if exclusions is None:\n",
    "        exclusions = {}\n",
    "    \n",
    "    alpha = adaptive_alpha(query_attrs)\n",
    "    ranked = []\n",
    "\n",
    "    for c in candidates:\n",
    "        meta = metadata_collection.get(\n",
    "            ids=[c[\"image_id\"]],\n",
    "            include=[\"metadatas\"]\n",
    "        )[\"metadatas\"][0]\n",
    "\n",
    "        # HARD FILTER: Skip items that match exclusions\n",
    "        should_exclude = False\n",
    "        for attr, excluded_value in exclusions.items():\n",
    "            meta_value = meta.get(attr)\n",
    "            \n",
    "            # Handle \"any\" exclusion (e.g., \"plain\" means no stones at all)\n",
    "            if excluded_value == \"any\":\n",
    "                # Exclude if has ANY stone (not unknown/null)\n",
    "                if meta_value and meta_value not in [\"unknown\", \"null\", \"\"]:\n",
    "                    should_exclude = True\n",
    "                    print(f\"üö´ Excluding {c['image_id']}: has {attr}={meta_value} (want none)\")\n",
    "                    break\n",
    "            # Handle specific exclusion\n",
    "            elif meta_value == excluded_value:\n",
    "                should_exclude = True\n",
    "                print(f\"üö´ Excluding {c['image_id']}: has {attr}={meta_value} (excluded)\")\n",
    "                break\n",
    "        \n",
    "        # Skip this item if it matches any exclusion\n",
    "        if should_exclude:\n",
    "            continue\n",
    "        \n",
    "        # Calculate positive boost from matching attributes\n",
    "        adjust = refined_metadata_adjustment(meta, query_attrs)\n",
    "        \n",
    "        # Final score: visual + metadata boost (no exclusion penalty needed)\n",
    "        final_score = c[\"visual_score\"] - alpha * adjust\n",
    "\n",
    "        ranked.append({\n",
    "            \"image_id\": c[\"image_id\"],\n",
    "            \"visual_score\": c[\"visual_score\"],\n",
    "            \"metadata_boost\": adjust,\n",
    "            \"final_score\": final_score\n",
    "        })\n",
    "\n",
    "    return sorted(ranked, key=lambda x: x[\"final_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d19025",
   "metadata": {},
   "source": [
    "## üéØ Cross-Encoder Re-Ranking\n",
    "\n",
    "**Function**: `rerank_with_cross_encoder(query, candidates, top_k)`\n",
    "\n",
    "**Two-Stage Retrieval**:\n",
    "1. **CLIP Bi-Encoder**: Fast retrieval (already done)\n",
    "2. **Cross-Encoder**: Accurate semantic re-ranking\n",
    "\n",
    "**Process**:\n",
    "1. Create query-document pairs (query + BLIP caption + metadata)\n",
    "2. Score all pairs with cross-encoder (batch processing)\n",
    "3. Combine scores: 30% visual + 20% metadata + 50% cross-encoder\n",
    "\n",
    "**Why cross-encoder?**\n",
    "- Better semantic understanding than CLIP alone\n",
    "- Captures nuanced relationships\n",
    "- Improves ranking quality significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aaf5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_with_cross_encoder(\n",
    "    query: str,\n",
    "    candidates: List[Dict],\n",
    "    top_k: int = 12\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Re-rank candidates using cross-encoder for better semantic matching.\n",
    "    \n",
    "    Two-stage pipeline:\n",
    "    1. CLIP bi-encoder: Fast retrieval (already done)\n",
    "    2. Cross-encoder: Accurate semantic re-ranking\n",
    "    \n",
    "    Args:\n",
    "        query: User query text\n",
    "        candidates: List of {image_id, visual_score, metadata_boost, ...}\n",
    "        top_k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        Re-ranked list of top K candidates\n",
    "    \"\"\"\n",
    "    if not candidates:\n",
    "        return []\n",
    "    \n",
    "    # Prepare query-document pairs for cross-encoder\n",
    "    pairs = []\n",
    "    for c in candidates:\n",
    "        # Get BLIP caption for this image\n",
    "        caption = BLIP_CAPTIONS.get(c[\"image_id\"], \"\")\n",
    "        \n",
    "        # Get metadata\n",
    "        meta = metadata_collection.get(\n",
    "            ids=[c[\"image_id\"]],\n",
    "            include=[\"metadatas\"]\n",
    "        )[\"metadatas\"][0]\n",
    "        \n",
    "        # Create rich text representation combining caption + metadata\n",
    "        doc_text = f\"{caption}. Category: {meta.get('category', 'unknown')}, Metal: {meta.get('metal', 'unknown')}, Stone: {meta.get('primary_stone', 'unknown')}\"\n",
    "        \n",
    "        pairs.append([query, doc_text])\n",
    "    \n",
    "    # Score all pairs with cross-encoder (batch processing)\n",
    "    print(f\"üîÑ Cross-encoder scoring {len(pairs)} candidates...\")\n",
    "    encoder = get_cross_encoder()\n",
    "    cross_scores = encoder.predict(pairs, batch_size=32)\n",
    "    \n",
    "    # Combine scores: visual + metadata + cross-encoder\n",
    "    for i, c in enumerate(candidates):\n",
    "        c[\"cross_encoder_score\"] = float(cross_scores[i])\n",
    "        \n",
    "        # Final score combines all signals\n",
    "        # - Visual similarity (CLIP): 30% weight\n",
    "        # - Metadata match: 20% weight  \n",
    "        # - Semantic similarity (cross-encoder): 50% weight (highest)\n",
    "        c[\"final_score_reranked\"] = (\n",
    "            -c[\"visual_score\"] * 0.3 +  # Negate because lower distance = better\n",
    "            c.get(\"metadata_boost\", 0) * 0.2 +\n",
    "            c[\"cross_encoder_score\"] * 0.5\n",
    "        )\n",
    "    \n",
    "    # Sort by final score (higher is better)\n",
    "    ranked = sorted(candidates, key=lambda x: x[\"final_score_reranked\"], reverse=True)\n",
    "    \n",
    "    print(f\"‚úÖ Re-ranked {len(ranked)} candidates, returning top {top_k}\")\n",
    "    return ranked[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed0f92b",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Image Upload & Encoding\n",
    "\n",
    "**Function**: `encode_uploaded_image(image_bytes) -> np.ndarray`\n",
    "\n",
    "**Process**:\n",
    "1. Load image from bytes\n",
    "2. Convert to RGB if needed\n",
    "3. Resize if too large (max 512x512)\n",
    "4. Preprocess for CLIP (224x224, normalized)\n",
    "5. Encode with CLIP image encoder\n",
    "6. Clean up GPU memory\n",
    "\n",
    "**Optimizations**:\n",
    "- Thumbnail resizing for large images\n",
    "- Explicit memory cleanup\n",
    "- Error handling with HTTP exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5788a1f2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def encode_uploaded_image(image_bytes: bytes) -> np.ndarray:\n",
    "    \"\"\"Encode uploaded image using CLIP model\"\"\"\n",
    "    try:\n",
    "        # Open image from bytes\n",
    "        image = Image.open(io.BytesIO(image_bytes))\n",
    "        \n",
    "        # Convert to RGB if necessary\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        # Resize if too large (max 512x512 for efficiency)\n",
    "        max_size = 512\n",
    "        if max(image.size) > max_size:\n",
    "            image.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)\n",
    "        \n",
    "        # Preprocess for CLIP\n",
    "        from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "        \n",
    "        preprocess = Compose([\n",
    "            Resize(224, interpolation=Image.BICUBIC),\n",
    "            CenterCrop(224),\n",
    "            ToTensor(),\n",
    "            Normalize((0.48145466, 0.4578275, 0.40821073), \n",
    "                     (0.26862954, 0.26130258, 0.27577711))\n",
    "        ])\n",
    "        \n",
    "        image_tensor = preprocess(image).unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        # Encode with CLIP\n",
    "        model = get_clip_model()\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image_tensor)\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            result = image_features.cpu().numpy()[0]\n",
    "        \n",
    "        # Memory cleanup\n",
    "        del image_tensor, image_features\n",
    "        if DEVICE == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=f\"Failed to process image: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8713e56d",
   "metadata": {},
   "source": [
    "## üì∏ OCR Text Extraction\n",
    "\n",
    "**Function**: `extract_text_from_image(image_bytes) -> str`\n",
    "\n",
    "**Two-Tier OCR Strategy**:\n",
    "1. **Primary**: NVIDIA NeMo Retriever OCR API\n",
    "2. **Fallback**: GPT-4.1-Nano (Navigate Labs)\n",
    "\n",
    "**Process**:\n",
    "1. Encode image to base64\n",
    "2. Call NVIDIA OCR API (15s timeout)\n",
    "3. If fails ‚Üí Call GPT-4.1-Nano fallback\n",
    "4. Return extracted text\n",
    "\n",
    "**Error Handling**:\n",
    "- Fast failover (15s timeout)\n",
    "- Detailed logging\n",
    "- HTTP exceptions for client feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d74af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_image(image_bytes: bytes) -> str:\n",
    "    \"\"\"Extract text from image using NVIDIA NeMo Retriever OCR API with GPT-4.1-Nano fallback\"\"\"\n",
    "    \n",
    "    # Try NVIDIA OCR first if key is configured\n",
    "    extracted_text = \"\"\n",
    "    nvidia_failed = False\n",
    "    \n",
    "    if NVIDIA_API_KEY:\n",
    "        try:\n",
    "            # Encode image to base64\n",
    "            image_b64 = base64.b64encode(image_bytes).decode()\n",
    "            \n",
    "            # Prepare request\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {NVIDIA_API_KEY}\",\n",
    "                \"Accept\": \"application/json\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            \n",
    "            payload = {\n",
    "                \"input\": [\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"url\": f\"data:image/png;base64,{image_b64}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            # Call NVIDIA OCR API\n",
    "            print(f\"üìû Calling NVIDIA OCR API...\")\n",
    "            response = requests.post(\n",
    "                NVIDIA_OCR_URL,\n",
    "                headers=headers,\n",
    "                json=payload,\n",
    "                timeout=15  # Shorter timeout to fail fast\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                \n",
    "                # Format 1: Text detections array\n",
    "                if \"data\" in result and isinstance(result[\"data\"], list) and len(result[\"data\"]) > 0:\n",
    "                    for data_item in result[\"data\"]:\n",
    "                        if isinstance(data_item, dict) and \"text_detections\" in data_item:\n",
    "                            for detection in data_item[\"text_detections\"]:\n",
    "                                if \"text_prediction\" in detection and \"text\" in detection[\"text_prediction\"]:\n",
    "                                    extracted_text += detection[\"text_prediction\"][\"text\"] + \" \"\n",
    "                        elif isinstance(data_item, dict) and \"content\" in data_item:\n",
    "                            extracted_text += data_item[\"content\"] + \" \"\n",
    "                \n",
    "                # Format 2: Direct text field\n",
    "                elif \"text\" in result:\n",
    "                    extracted_text = result[\"text\"]\n",
    "                \n",
    "                # Format 3: Choices/Results\n",
    "                elif \"choices\" in result and len(result[\"choices\"]) > 0:\n",
    "                    if \"text\" in result[\"choices\"][0]:\n",
    "                        extracted_text = result[\"choices\"][0][\"text\"]\n",
    "                    elif \"message\" in result[\"choices\"][0]:\n",
    "                        extracted_text = result[\"choices\"][0][\"message\"].get(\"content\", \"\")\n",
    "                \n",
    "                extracted_text = extracted_text.strip()\n",
    "                if extracted_text:\n",
    "                    print(f\"‚úÖ Extracted text (NVIDIA): '{extracted_text}'\")\n",
    "                    return extracted_text\n",
    "            \n",
    "            print(f\"‚ö†Ô∏è NVIDIA OCR failed with status {response.status_code}. Trying fallback...\")\n",
    "            nvidia_failed = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è NVIDIA OCR exception: {e}. Trying fallback...\")\n",
    "            nvidia_failed = True\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è NVIDIA_API_KEY not set. Using fallback directly.\")\n",
    "        nvidia_failed = True\n",
    "    \n",
    "    # FALLBACK: Custom GPT-4.1-Nano OCR (OpenAI Compatible)\n",
    "    try:\n",
    "        if not OPENAI_API_KEY:\n",
    "             raise HTTPException(status_code=500, detail=\"OCR unavailable: Primary failed and OPENAI_API_KEY not set for fallback.\")\n",
    "             \n",
    "        print(\"üîÑ Using Global GPT-4.1-Nano Fallback...\")\n",
    "        \n",
    "        # Initialize OpenAI client with custom base URL\n",
    "        from openai import OpenAI\n",
    "        \n",
    "        client = OpenAI(\n",
    "            base_url=\"https://apidev.navigatelabsai.com/v1\",\n",
    "            api_key=OPENAI_API_KEY\n",
    "        )\n",
    "        \n",
    "        image_b64 = base64.b64encode(image_bytes).decode()\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"Transcribe the handwritten text in this image exactly as it appears. Output ONLY the text, nothing else.\"},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/png;base64,{image_b64}\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        extracted_text = response.choices[0].message.content.strip()\n",
    "        \n",
    "        if not extracted_text:\n",
    "             raise HTTPException(status_code=400, detail=\"No readable text found in image (Fallback).\")\n",
    "             \n",
    "        print(f\"‚úÖ Extracted text (Fallback GPT): '{extracted_text}'\")\n",
    "        return extracted_text\n",
    "        \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fallback OCR failed: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"OCR failed permanently: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e48720",
   "metadata": {},
   "source": [
    "## üí¨ LLM-Powered Explanation Generation (Batch Processing)\n",
    "\n",
    "**Function**: `batch_generate_explanations(results, query_attrs, user_query)`\n",
    "\n",
    "**Strategy**: Generate ALL explanations in ONE API call\n",
    "\n",
    "**Process**:\n",
    "1. Build compact context for all items (up to 20)\n",
    "2. Single LLM call with numbered format\n",
    "3. Parse numbered responses\n",
    "4. Fallback for missing explanations\n",
    "\n",
    "**Benefits**:\n",
    "- 10x faster than individual calls\n",
    "- Lower API costs\n",
    "- Better consistency across explanations\n",
    "\n",
    "**Fallback**: Template-based explanations if LLM unavailable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c484025a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def batch_generate_explanations(results: List[Dict], query_attrs: Dict, user_query: str) -> List[str]:\n",
    "    \"\"\"Generate diverse, LLM-powered explanations for all search results in ONE API call\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        return []\n",
    "    \n",
    "    # Build context for all items (handle up to 20 items in one call)\n",
    "    items_context = []\n",
    "    for idx, r in enumerate(results, 1):\n",
    "        meta = metadata_collection.get(\n",
    "            ids=[r[\"image_id\"]],\n",
    "            include=[\"metadatas\"]\n",
    "        )[\"metadatas\"][0]\n",
    "        \n",
    "        matched_attrs = [v for k, v in query_attrs.items() if meta.get(k) == v]\n",
    "        \n",
    "        # Compact format to save tokens\n",
    "        item_info = f\"{idx}. {meta.get('category', 'item')} | {meta.get('metal', '?')} | {meta.get('primary_stone', '?')} | score:{r['visual_score']:.2f} | matched:{','.join(matched_attrs) if matched_attrs else 'none'}\"\n",
    "        items_context.append(item_info)\n",
    "    \n",
    "    # Compact prompt to fit more items\n",
    "    prompt = f\"\"\"Query: \"{user_query}\"\n",
    "\n",
    "Write 1 brief sentence for EACH item:\n",
    "\n",
    "{chr(10).join(items_context)}\n",
    "\n",
    "Format:\n",
    "1. [sentence]\n",
    "2. [sentence]\n",
    "etc.\"\"\"\n",
    "\n",
    "    if groq_client is None:\n",
    "        explanations = []\n",
    "    else:\n",
    "        try:\n",
    "            # Single API call for ALL items\n",
    "            response = groq_client.chat.completions.create(\n",
    "                model=\"llama-3.1-8b-instant\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Write brief jewellery recommendations.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.7,\n",
    "                max_tokens=min(800, len(results) * 60),  # Increased for 12+ items\n",
    "                top_p=0.9\n",
    "            )\n",
    "\n",
    "            # Parse response\n",
    "            full_response = response.choices[0].message.content.strip()\n",
    "            explanations = []\n",
    "\n",
    "            import re\n",
    "            pattern = r'^\\s*(\\d+)[\\.:)\\-]\\s*(.+?)(?=^\\s*\\d+[\\.:)\\-]|\\Z)'\n",
    "            matches = re.findall(pattern, full_response, re.MULTILINE | re.DOTALL)\n",
    "\n",
    "            if matches and len(matches) >= len(results):\n",
    "                for num, text in matches[:len(results)]:\n",
    "                    clean_text = ' '.join(text.strip().split())\n",
    "                    if clean_text and len(clean_text) > 10:\n",
    "                        explanations.append(clean_text)\n",
    "\n",
    "            if len(explanations) >= len(results):\n",
    "                return explanations[:len(results)]\n",
    "\n",
    "            # If incomplete, pad with fallback\n",
    "            print(f\"‚ö†Ô∏è LLM returned {len(explanations)}/{len(results)} explanations, padding with fallback\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è LLM explanation failed: {e}, using fallback\")\n",
    "            explanations = []\n",
    "    \n",
    "    # Fallback for missing explanations\n",
    "    while len(explanations) < len(results):\n",
    "        idx = len(explanations)\n",
    "        r = results[idx]\n",
    "        meta = metadata_collection.get(\n",
    "            ids=[r[\"image_id\"]],\n",
    "            include=[\"metadatas\"]\n",
    "        )[\"metadatas\"][0]\n",
    "        matched_attrs = [v for k, v in query_attrs.items() if meta.get(k) == v]\n",
    "        \n",
    "        category = meta.get('category', 'item')\n",
    "        metal = meta.get('metal', 'unknown')\n",
    "        stone = meta.get('primary_stone', 'unknown')\n",
    "        \n",
    "        if matched_attrs and r['visual_score'] < 1.3:\n",
    "            explanations.append(\n",
    "                f\"Excellent {category} featuring {' and '.join(matched_attrs)}. High visual similarity (score: {r['visual_score']:.2f}).\"\n",
    "            )\n",
    "        elif matched_attrs:\n",
    "            explanations.append(\n",
    "                f\"Beautiful {metal} {category} with {stone}. Features {' and '.join(matched_attrs)}.\"\n",
    "            )\n",
    "        elif r['visual_score'] < 1.3:\n",
    "            explanations.append(\n",
    "                f\"Highly similar {category} with excellent visual match. {metal.capitalize()} with {stone}.\"\n",
    "            )\n",
    "        else:\n",
    "            explanations.append(\n",
    "                f\"Recommended {metal} {category} with {stone}. Good visual similarity.\"\n",
    "            )\n",
    "    \n",
    "    return explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135b1a69",
   "metadata": {},
   "source": [
    "## üåê API Endpoints\n",
    "\n",
    "**Available endpoints**:\n",
    "- `GET /health`: Health check and model status\n",
    "- `POST /search/text`: Text-based search\n",
    "- `POST /search/similar`: Find similar items\n",
    "- `POST /search/upload-image`: Search by uploaded image\n",
    "- `POST /search/ocr-query`: Search by OCR-extracted text\n",
    "- `GET /image/{image_id}`: Serve product images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9be8f9",
   "metadata": {},
   "source": [
    "### Health Check Endpoint\n",
    "\n",
    "**Purpose**: Monitor service health and model loading status\n",
    "\n",
    "**Returns**:\n",
    "- Service status\n",
    "- Model loading status (CLIP, Cross-Encoder, BLIP)\n",
    "- Database counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453fe7cd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@app.get(\"/health\")\n",
    "def health_check():\n",
    "    \"\"\"Health check endpoint for HF Spaces monitoring\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"models_loaded\": {\n",
    "            \"clip\": clip_model is not None,\n",
    "            \"cross_encoder\": cross_encoder is not None,\n",
    "            \"blip_captions\": len(BLIP_CAPTIONS) > 0\n",
    "        },\n",
    "        \"database\": {\n",
    "            \"images\": image_collection.count(),\n",
    "            \"metadata\": metadata_collection.count()\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92061e31",
   "metadata": {},
   "source": [
    "### Text Search Endpoint\n",
    "\n",
    "**Endpoint**: `POST /search/text`\n",
    "\n",
    "**Pipeline**:\n",
    "1. Detect intent and attributes from query (LLM)\n",
    "2. Apply explicit UI filters (if provided)\n",
    "3. Retrieve visual candidates (CLIP)\n",
    "4. Filter by valid IDs (metadata collection)\n",
    "5. Re-rank with cross-encoder (optional)\n",
    "6. Generate explanations (optional, batch LLM)\n",
    "\n",
    "**Two modes**:\n",
    "- **Filter only**: No text query, just return filtered items\n",
    "- **Text + Filter**: Combine vector search with metadata filtering\n",
    "\n",
    "**Optimizations**:\n",
    "- Fetch K*5 candidates when filtering (ensure intersections)\n",
    "- Manual filtering in Python (image collection lacks metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e520f6e6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@app.post(\"/search/text\")\n",
    "def search_text(req: TextSearchRequest):\n",
    "    # Detect intent from text\n",
    "    if req.query.strip():\n",
    "        intent = detect_intent_and_attributes(req.query)\n",
    "        attrs = intent[\"attributes\"]\n",
    "    else:\n",
    "        intent = {\"intent\": \"filter\", \"attributes\": {}, \"exclusions\": {}}\n",
    "        attrs = {}\n",
    "\n",
    "    # === DUAL-STAGE FILTERING STRATEGY ===\n",
    "    # 1. Identify valid IDs from Metadata Collection (Source of Truth)\n",
    "    # 2. Use those IDs to filter Vector Search results\n",
    "\n",
    "    # Construct WHERE clause for Metadata Collection\n",
    "    where_clauses = []\n",
    "    \n",
    "    if req.filters:\n",
    "        for key, value in req.filters.items():\n",
    "            where_clauses.append({key: value.lower()}) # Explicit filters\n",
    "    \n",
    "    # Also apply attributes detected from text as generic filters if user didn't specify explicit ones\n",
    "    # (Optional: this makes \"emerald ring\" implies primary_stone=emerald)\n",
    "    # But usually we let visual search handle text unless it's strict.\n",
    "    \n",
    "    final_where = None\n",
    "    if len(where_clauses) > 1:\n",
    "        final_where = {\"$and\": where_clauses}\n",
    "    elif len(where_clauses) == 1:\n",
    "        final_where = where_clauses[0]\n",
    "    \n",
    "    valid_ids = None\n",
    "    if final_where:\n",
    "        # Fetch ALL valid IDs matching the filter\n",
    "        print(f\"üîç Filtering metadata with: {final_where}\")\n",
    "        meta_res = metadata_collection.get(where=final_where, include=[\"metadatas\"])\n",
    "        if meta_res[\"ids\"]:\n",
    "            valid_ids = set(meta_res[\"ids\"])\n",
    "            print(f\"‚úÖ Found {len(valid_ids)} valid items matching filters.\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No items match the filters.\")\n",
    "            return {\"query\": req.query, \"intent\": attrs, \"results\": []}\n",
    "\n",
    "    # === EXECUTE SEARCH ===\n",
    "    \n",
    "    # Case A: Filter Only (No Text Query)\n",
    "    if not req.query.strip() and valid_ids:\n",
    "        # Just return the matching items (Top K)\n",
    "        candidates = [{\"image_id\": vid, \"visual_score\": 0.0} for vid in list(valid_ids)[:req.top_k]]\n",
    "        ranked = candidates # No ranking needed without text\n",
    "        explanations = [\"Filtered result\"] * len(ranked)\n",
    "        \n",
    "    # Case B: Text Query (with or without Filter)\n",
    "    else:\n",
    "        search_query = req.query if req.query.strip() else \"jewellery\"\n",
    "        \n",
    "        # We perform a BROADER vector search, then filter in Python\n",
    "        # Retrieve K*5 or at least 100 to ensure we find intersections\n",
    "        fetch_k = 200 if valid_ids else 40 \n",
    "        \n",
    "        # Note: We do NOT pass 'where' to retrieve_visual_candidates because \n",
    "        # image_collection lacks metadata. We filter manually.\n",
    "        candidates = retrieve_visual_candidates(search_query, k=fetch_k)\n",
    "        \n",
    "        filtered_candidates = []\n",
    "        for c in candidates:\n",
    "            if valid_ids is not None:\n",
    "                if c[\"image_id\"] in valid_ids:\n",
    "                    filtered_candidates.append(c)\n",
    "            else:\n",
    "                filtered_candidates.append(c)\n",
    "        \n",
    "        # Apply strict limit now\n",
    "        filtered = filtered_candidates # apply_metadata_boost(filtered_candidates, attrs, {})\n",
    "        \n",
    "        # Cross-encoder re-ranking\n",
    "        if req.use_reranking and filtered and req.query.strip():\n",
    "            ranked = rerank_with_cross_encoder(req.query, filtered, req.top_k)\n",
    "        else:\n",
    "            ranked = filtered[:req.top_k]\n",
    "            \n",
    "        # Explanations\n",
    "        if req.use_explanations and req.query.strip():\n",
    "            explanations = batch_generate_explanations(ranked, attrs, search_query)\n",
    "        else:\n",
    "            explanations = [\"Match found\"] * len(ranked)\n",
    "\n",
    "    # === FORMAT RESULTS ===\n",
    "    results = []\n",
    "    \n",
    "    # Fetch metadata for final results\n",
    "    if ranked:\n",
    "        ranked_ids = [r[\"image_id\"] for r in ranked]\n",
    "        metas = metadata_collection.get(ids=ranked_ids, include=[\"metadatas\"])[\"metadatas\"]\n",
    "        meta_map = {rid: m for rid, m in zip(ranked_ids, metas)}\n",
    "    else:\n",
    "        meta_map = {}\n",
    "\n",
    "    for r, explanation in zip(ranked, explanations):\n",
    "        results.append({\n",
    "            \"image_id\": r[\"image_id\"],\n",
    "            \"explanation\": explanation,\n",
    "            \"metadata\": meta_map.get(r[\"image_id\"], {}),\n",
    "            \"scores\": {\n",
    "                \"visual\": r[\"visual_score\"],\n",
    "                \"final\": r.get(\"visual_score\", 0) # simplified\n",
    "            }\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"query\": req.query,\n",
    "        \"intent\": attrs,\n",
    "        \"results\": results\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd25f1c",
   "metadata": {},
   "source": [
    "### Similar Items Search Endpoint\n",
    "\n",
    "**Endpoint**: `POST /search/similar`\n",
    "\n",
    "**Pipeline**:\n",
    "1. Get base image embedding from database\n",
    "2. Query for similar images (vector similarity)\n",
    "3. Extract attributes from base image metadata\n",
    "4. Apply metadata boost to candidates\n",
    "5. Generate explanations (batch LLM)\n",
    "\n",
    "**Use case**: \"Find similar items\" feature on product pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01766841",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@app.post(\"/search/similar\")\n",
    "def search_similar(req: SimilarSearchRequest):\n",
    "    base = image_collection.get(\n",
    "        ids=[req.image_id],\n",
    "        include=[\"embeddings\"]\n",
    "    )[\"embeddings\"][0]\n",
    "\n",
    "    res = image_collection.query(\n",
    "        query_embeddings=[base],\n",
    "        n_results=req.top_k + 1\n",
    "    )\n",
    "\n",
    "    base_meta = metadata_collection.get(\n",
    "        ids=[req.image_id],\n",
    "        include=[\"metadatas\"]\n",
    "    )[\"metadatas\"][0]\n",
    "\n",
    "    attrs = {\n",
    "        k: base_meta[k]\n",
    "        for k in [\"category\", \"metal\", \"primary_stone\"]\n",
    "        if base_meta.get(k) != \"unknown\"\n",
    "    }\n",
    "\n",
    "    candidates = [\n",
    "        {\n",
    "            \"image_id\": img_id,\n",
    "            \"visual_score\": dist\n",
    "        }\n",
    "        for img_id, dist in zip(res[\"ids\"][0], res[\"distances\"][0])\n",
    "        if img_id != req.image_id\n",
    "    ]\n",
    "\n",
    "    ranked = apply_metadata_boost(candidates, attrs, {})[:req.top_k]\n",
    "\n",
    "    # Generate all explanations in one batch LLM call\n",
    "    # For similar search, use the base image ID as the query context\n",
    "    query_context = f\"items similar to {req.image_id}\"\n",
    "    explanations = batch_generate_explanations(ranked, attrs, query_context)\n",
    "    \n",
    "    results = []\n",
    "    for r, explanation in zip(ranked, explanations):\n",
    "        results.append({\n",
    "            \"image_id\": r[\"image_id\"],\n",
    "            \"explanation\": explanation,\n",
    "            \"scores\": {\n",
    "                \"visual\": r[\"visual_score\"],\n",
    "                \"metadata\": r[\"metadata_boost\"],\n",
    "                \"final\": r[\"final_score\"]\n",
    "            }\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"base_image\": req.image_id,\n",
    "        \"results\": results\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cf0cc1",
   "metadata": {},
   "source": [
    "### Image Upload Search Endpoint\n",
    "\n",
    "**Endpoint**: `POST /search/upload-image`\n",
    "\n",
    "**Pipeline**:\n",
    "1. Validate file type (must be image)\n",
    "2. Encode uploaded image with CLIP\n",
    "3. Query ChromaDB for similar images\n",
    "4. Infer attributes from top result\n",
    "5. Apply metadata boost\n",
    "6. Generate explanations (batch LLM)\n",
    "\n",
    "**Use case**: \"Search by image\" feature (upload your own photo)\n",
    "\n",
    "**Validations**:\n",
    "- File type check\n",
    "- Size limit (5MB via middleware)\n",
    "- Image format conversion (RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4417145",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/search/upload-image\")\n",
    "async def search_by_uploaded_image(\n",
    "    file: UploadFile = File(...),\n",
    "    top_k: int = 12\n",
    "):\n",
    "    \"\"\"\n",
    "    Search for similar jewellery items by uploading an image.\n",
    "    The image is encoded using CLIP and queried against the database.\n",
    "    \"\"\"\n",
    "    # Validate file type\n",
    "    if not file.content_type or not file.content_type.startswith('image/'):\n",
    "        raise HTTPException(status_code=400, detail=\"File must be an image\")\n",
    "    \n",
    "    try:\n",
    "        # Read image bytes\n",
    "        image_bytes = await file.read()\n",
    "        \n",
    "        # Encode image with CLIP\n",
    "        query_embedding = encode_uploaded_image(image_bytes)\n",
    "        \n",
    "        # Query ChromaDB\n",
    "        res = image_collection.query(\n",
    "            query_embeddings=[query_embedding.tolist()],\n",
    "            n_results=min(100, top_k * 10),\n",
    "            include=[\"distances\"]\n",
    "        )\n",
    "        \n",
    "        # Get metadata for all results\n",
    "        candidates = []\n",
    "        for img_id, dist in zip(res[\"ids\"][0], res[\"distances\"][0]):\n",
    "            candidates.append({\n",
    "                \"image_id\": img_id,\n",
    "                \"visual_score\": dist\n",
    "            })\n",
    "        \n",
    "        # Get metadata from first result to infer attributes\n",
    "        if candidates:\n",
    "            base_meta = metadata_collection.get(\n",
    "                ids=[candidates[0][\"image_id\"]],\n",
    "                include=[\"metadatas\"]\n",
    "            )[\"metadatas\"][0]\n",
    "            \n",
    "            attrs = {\n",
    "                k: base_meta[k]\n",
    "                for k in [\"category\", \"metal\", \"primary_stone\"]\n",
    "                if base_meta.get(k) != \"unknown\"\n",
    "            }\n",
    "        else:\n",
    "            attrs = {}\n",
    "        \n",
    "        # Apply metadata boost (no exclusions for image upload)\n",
    "        ranked = apply_metadata_boost(candidates, attrs, {})[:top_k]\n",
    "        \n",
    "        # Generate explanations in batch\n",
    "        query_context = f\"items visually similar to uploaded image\"\n",
    "        explanations = batch_generate_explanations(ranked, attrs, query_context)\n",
    "        \n",
    "        results = []\n",
    "        for r, explanation in zip(ranked, explanations):\n",
    "            results.append({\n",
    "                \"image_id\": r[\"image_id\"],\n",
    "                \"explanation\": explanation,\n",
    "                \"scores\": {\n",
    "                    \"visual\": r[\"visual_score\"],\n",
    "                    \"metadata\": r[\"metadata_boost\"],\n",
    "                    \"final\": r[\"final_score\"]\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"query_type\": \"uploaded_image\",\n",
    "            \"filename\": file.filename,\n",
    "            \"results\": results\n",
    "        }\n",
    "        \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Image search failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332e4eef",
   "metadata": {},
   "source": [
    "### OCR Query Search Endpoint\n",
    "\n",
    "**Endpoint**: `POST /search/ocr-query`\n",
    "\n",
    "**Pipeline**:\n",
    "1. Validate file type (must be image)\n",
    "2. Extract text using NVIDIA OCR (or GPT-4.1-Nano fallback)\n",
    "3. Detect intent and attributes from extracted text (LLM)\n",
    "4. Retrieve visual candidates (CLIP)\n",
    "5. Apply metadata boost + exclusion filtering\n",
    "6. Re-rank with cross-encoder\n",
    "7. Generate explanations (batch LLM)\n",
    "\n",
    "**Use case**: \"Search by handwritten note\" feature\n",
    "\n",
    "**Example**: User uploads photo of note saying \"gold ring with diamonds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283a28a7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@app.post(\"/search/ocr-query\")\n",
    "async def search_by_ocr_query(\n",
    "    file: UploadFile = File(...),\n",
    "    top_k: int = 12\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract text from uploaded image using NVIDIA NeMo OCR,\n",
    "    then perform text-based search with the extracted query.\n",
    "    \"\"\"\n",
    "    # Validate file type\n",
    "    if not file.content_type or not file.content_type.startswith('image/'):\n",
    "        raise HTTPException(status_code=400, detail=\"File must be an image\")\n",
    "    \n",
    "    try:\n",
    "        # Read image bytes\n",
    "        image_bytes = await file.read()\n",
    "        \n",
    "        # Extract text using NVIDIA OCR\n",
    "        extracted_text = extract_text_from_image(image_bytes)\n",
    "        \n",
    "        print(f\"üìù Extracted text from image: '{extracted_text}'\")\n",
    "        \n",
    "        # Use the extracted text for normal text search\n",
    "        intent = detect_intent_and_attributes(extracted_text)\n",
    "        attrs = intent[\"attributes\"]\n",
    "        exclusions = intent.get(\"exclusions\", {})\n",
    "        \n",
    "        # Stage 1: CLIP retrieval (reduced to k=40 for HF Spaces)\n",
    "        candidates = retrieve_visual_candidates(extracted_text, k=40)\n",
    "        \n",
    "        # Stage 2: Metadata boost + exclusion filtering\n",
    "        filtered = apply_metadata_boost(candidates, attrs, exclusions)\n",
    "        \n",
    "        # Stage 3: Cross-encoder re-ranking\n",
    "        ranked = rerank_with_cross_encoder(extracted_text, filtered, top_k)\n",
    "        \n",
    "        # Generate explanations in batch\n",
    "        explanations = batch_generate_explanations(ranked, attrs, extracted_text)\n",
    "        \n",
    "        results = []\n",
    "        for r, explanation in zip(ranked, explanations):\n",
    "            results.append({\n",
    "                \"image_id\": r[\"image_id\"],\n",
    "                \"explanation\": explanation,\n",
    "                \"scores\": {\n",
    "                    \"visual\": r[\"visual_score\"],\n",
    "                    \"metadata\": r[\"metadata_boost\"],\n",
    "                    \"final\": r[\"final_score\"]\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            \"query_type\": \"ocr_extracted\",\n",
    "            \"extracted_text\": extracted_text,\n",
    "            \"intent\": attrs,\n",
    "            \"results\": results\n",
    "        }\n",
    "        \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"OCR search failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cf3cf8",
   "metadata": {},
   "source": [
    "### Image Serving Endpoint\n",
    "\n",
    "**Endpoint**: `GET /image/{image_id}`\n",
    "\n",
    "**Purpose**: Serve product images from local filesystem\n",
    "\n",
    "**Returns**: Image file (JPEG/PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a982e0a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@app.get(\"/image/{image_id}\")\n",
    "def get_image(image_id: str):\n",
    "    path = os.path.join(IMAGE_DIR, image_id)\n",
    "    if not os.path.exists(path):\n",
    "        raise HTTPException(status_code=404, detail=\"Image not found\")\n",
    "    return FileResponse(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb562e1",
   "metadata": {},
   "source": [
    "## üöÄ Server Startup\n",
    "\n",
    "**Development server**: Uvicorn on port 8000\n",
    "\n",
    "**Configuration**:\n",
    "- Host: 0.0.0.0 (accept external connections)\n",
    "- Port: 8000\n",
    "- Log level: info\n",
    "\n",
    "**API Documentation**: Available at http://localhost:8000/docs (Swagger UI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfecf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    print(\"üöÄ Starting Jewellery Search API server...\")\n",
    "    print(f\"üìÅ Data directory: {DATA_DIR}\")\n",
    "    print(f\"üìÅ Image directory: {IMAGE_DIR}\")\n",
    "    print(f\"üìÅ ChromaDB path: {CHROMA_PATH}\")\n",
    "    print(f\"üåê Server will run on: http://localhost:8000\")\n",
    "    print(f\"üìñ API docs available at: http://localhost:8000/docs\")\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cf83e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
